# Information Theory

##  Entropy

$$
S=k\ln W=k\ln2\log_2W
$$

$W$ is the number of microscopic states/configuration, complexions according to Plank’s words. $\log_2W​$ is the amount of information. Information is change in entropy. Entropy is the information we do not have. When we find something out about a system, the entropy gets less. For people having different amount or kinds of information towards a certain system, entropy of the system is different for them.

In thermal dynamics, temperature is defined by the change in entropy relative to the change in inner energy.
$$
\frac{1}{T}=\frac{\partial S}{\partial E}
$$
where change in energy is defined by heat and heat is derived from the heat capacity.
$$
C=\frac{\partial T}{\partial E}=-T^2\frac{\partial^2S}{\partial E^2}
$$
There are systems holding negative heat capacity, gravitational systems, for example. In a gravitational system, as in all other kinds of classic physics systems, there is a certain relation between the kinetic energy and potential energy.
$$
\bar{E_k}=-\frac{1}{2}\bar{E_U},E=-E_k\propto NT
$$
Thereby gives negative heat capacity. In a galaxy, when taking away some planets, the other planets get closer to the center and move faster than before, thus increases the total kinetic energy.

Code is a systematic way of representing information.

